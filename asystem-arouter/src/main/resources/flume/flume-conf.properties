###############################################################################
#
# Flume Configuration
#
###############################################################################
#
agent.sources=mqtt_model_1
agent.channels=file_model_1
agent.sinks=s3_model_1
#
agent.sources.mqtt_model_1.type=com.cloudera.framework.common.flume.MqttSource
agent.sources.mqtt_model_1.brokerAccess=$MQTT_ACCESS_KEY
agent.sources.mqtt_model_1.brokerSecret=$MQTT_SECRET_KEY
agent.sources.mqtt_model_1.journalDir=$FLUME_MQTT_JOURNAL_DIR
agent.sources.mqtt_model_1.providerURL=tcp://$MQTT_BROKER_HOST:$MQTT_BROKER_PORT
agent.sources.mqtt_model_1.destinationName=asystem/+/anode/+/datum/1000
agent.sources.mqtt_model_1.backoffSleepIncrement=1
agent.sources.mqtt_model_1.maxBackoffSleep=10
agent.sources.mqtt_model_1.interceptors=metadata
agent.sources.mqtt_model_1.interceptors.metadata.type=com.jag.asystem.arouter.MetadataInterceptor$Builder
agent.sources.mqtt_model_1.interceptors.metadata.batchSize=$MQTT_BATCHSIZE
agent.sources.mqtt_model_1.interceptors.metadata.avroSchemaURL=$AVRO_SCHEMA_URL/1000/datum.avsc
agent.sources.mqtt_model_1.channels=file_model_1
#
agent.channels.file_model_1.type=file
agent.channels.file_model_1.checkpointDir=$FLUME_MQTT_CHECKPOINT_DIR
agent.channels.file_model_1.dataDirs=$FLUME_MQTT_DATA_DIRS
agent.channels.file_model_1.capacity=1000000
agent.channels.file_model_1.keep-alive=600
agent.channels.file_model_1.transactionCapacity=10000
agent.channels.file_model_1.checkpointInterval=30000
agent.channels.file_model_1.minimumRequiredSpace=1000000000
#
agent.sinks.s3_model_1.type=hdfs
agent.sinks.s3_model_1.hdfs.path=$S3_URL/%{ipt}/$S3_APP/staged/canonical/avro/avro/snappy/datum/1000/ingest_id=%{iid}/ingest_timestamp=%{its}
agent.sinks.s3_model_1.hdfs.filePrefix=datum
agent.sinks.s3_model_1.hdfs.fileSuffix=.avro
agent.sinks.s3_model_1.hdfs.inUsePrefix=.
agent.sinks.s3_model_1.hdfs.rollCount=0
agent.sinks.s3_model_1.hdfs.rollInterval=0
agent.sinks.s3_model_1.hdfs.rollSize=0
agent.sinks.s3_model_1.hdfs.idleTimeout=600
agent.sinks.s3_model_1.hdfs.batchSize=$HDFS_BATCHSIZE
agent.sinks.s3_model_1.hdfs.useLocalTimeStamp=true
agent.sinks.s3_model_1.hdfs.fileType=DataStream
agent.sinks.s3_model_1.serializer=org.apache.flume.sink.hdfs.AvroEventSerializer$Builder
agent.sinks.s3_model_1.serializer.compressionCodec=snappy
agent.sinks.s3_model_1.channel=file_model_1
#
